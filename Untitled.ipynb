{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/QxS++OdgNDD1voZRzndx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekevgin/-Python/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "rKGG0wpE5FIt"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as p\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "      \n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.activation = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        if self.activation==\"relu\":\n",
        "            return F.relu(x)\n",
        "        if self.activation==\"sigmoid\":\n",
        "            return F.sigmoid(x)\n",
        "        raise RuntimeError\n",
        "        \n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
        "        self.fc1 = Perceptron(input_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dp = nn.Dropout(0.25)\n",
        "        self.fc2 = Perceptron(hidden_dim, 10, \"relu\")\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 13)\n",
        "  dataset = BostonDataset(X_train, y_train)\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  mlp = MLP()\n",
        "  loss_function = nn.L1Loss()\n",
        "  #optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
        "  #optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
        "  optimizer = torch.optim.RMSprop(mlp.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False, foreach=None, maximize=False, differentiable=False)\n",
        "\n",
        "  for epoch in range(0, 5): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mlp(inputs)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeIoF5zz-exy",
        "outputId": "da49a618-7cc6-4640-873a-400942a67717"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after mini-batch     1: 0.039\n",
            "Loss after mini-batch    11: 0.202\n",
            "Loss after mini-batch    21: 0.097\n",
            "Loss after mini-batch    31: 0.088\n",
            "Starting epoch 2\n",
            "Loss after mini-batch     1: 0.006\n",
            "Loss after mini-batch    11: 0.069\n",
            "Loss after mini-batch    21: 0.078\n",
            "Loss after mini-batch    31: 0.081\n",
            "Starting epoch 3\n",
            "Loss after mini-batch     1: 0.008\n",
            "Loss after mini-batch    11: 0.058\n",
            "Loss after mini-batch    21: 0.062\n",
            "Loss after mini-batch    31: 0.074\n",
            "Starting epoch 4\n",
            "Loss after mini-batch     1: 0.005\n",
            "Loss after mini-batch    11: 0.058\n",
            "Loss after mini-batch    21: 0.068\n",
            "Loss after mini-batch    31: 0.046\n",
            "Starting epoch 5\n",
            "Loss after mini-batch     1: 0.006\n",
            "Loss after mini-batch    11: 0.053\n",
            "Loss after mini-batch    21: 0.055\n",
            "Loss after mini-batch    31: 0.050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#оптимизатор SGD дает loss 0.073\n",
        "#оптимизатор Adam дает loss 0.093\n",
        "#оптимизатор RMSprop дает loss 0.05"
      ],
      "metadata": {
        "id": "xwpNnk6N2HdL"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class BostonDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "      \n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.activation = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        if self.activation==\"relu\":\n",
        "            return F.relu(x)\n",
        "        if self.activation==\"sigmoid\":\n",
        "            return F.sigmoid(x)\n",
        "        raise RuntimeError\n",
        "        \n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
        "        self.fc1 = Perceptron(input_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dp = nn.Dropout(0.25)\n",
        "        self.fc2 = Perceptron(hidden_dim, 10, \"relu\")\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  X, y = load_boston(return_X_y=True)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 13)\n",
        "  dataset = BostonDataset(X_test, y_test)\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  mlp = MLP()\n",
        "  loss_function = nn.L1Loss()\n",
        "  #optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
        "  #optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
        "  optimizer = torch.optim.RMSprop(mlp.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False, foreach=None, maximize=False, differentiable=False)\n",
        "  for epoch in range(0, 5):\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mlp(inputs)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "      if i % 10 == 0:\n",
        "          print('Loss %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982633ca-cfc3-4839-8bcb-09b9f51c9bcc",
        "id": "ueCTAE5RKeOT"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss     1: 0.045\n",
            "Loss    11: 0.199\n",
            "Starting epoch 2\n",
            "Loss     1: 0.009\n",
            "Loss    11: 0.111\n",
            "Starting epoch 3\n",
            "Loss     1: 0.005\n",
            "Loss    11: 0.086\n",
            "Starting epoch 4\n",
            "Loss     1: 0.001\n",
            "Loss    11: 0.084\n",
            "Starting epoch 5\n",
            "Loss     1: 0.006\n",
            "Loss    11: 0.071\n"
          ]
        }
      ]
    }
  ]
}