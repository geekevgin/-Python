{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled36.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5BYdoZnsDi60nrgOJQ6cE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekevgin/-Python/blob/main/Untitled36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR0L4Rmq0K1K"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from linear_regression import linear_regression\n",
        "from sklearn.linear_model import LinearRegression as sklearn_LR\n",
        "%matplotlib inline\n",
        "\n",
        "## Переобученность модели\n",
        "\n",
        "def f(x):\n",
        "    return 0.6 - 13.2 * x - 5.3 * x ** 2 - 4.17 * x ** 3\n",
        "dots = np.linspace(-10, 10, 100)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "\n",
        "def f(x):\n",
        "    return 0.6 - 13.2 * x - 5.3 * x ** 2 - 4.17 * x ** 3\n",
        "np.random.seed(16)\n",
        "x_data = np.random.uniform(-10, 10, size=(10,))\n",
        "f_data = [f(i) for i in x_data] + np.random.uniform(-1000, 1000, size=(10,))\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "x_data\n",
        "\n",
        "f_data\n",
        "\n",
        "np.hstack([np.array([[1, 2]]), np.array([[3, 4]])])\n",
        "\n",
        "class polynomial_regression(linear_regression):\n",
        "    def __init__(self, max_power, *args, **kwargs):\n",
        "        self.max_power=max_power\n",
        "        super().__init__(*args, **kwargs)\n",
        "    @staticmethod\n",
        "    def generate_features(x, max_power):\n",
        "        x=x[:, np.newaxis]\n",
        "        return np.concatenate([x**i for i in range(1, max_power+1)], axis=1)\n",
        "    def fit(self, x, y):\n",
        "        super().fit(self.generate_features(x, self.max_power), y)\n",
        "    def predict(self, x):\n",
        "        return super().predict(self.generate_features(x, self.max_power))\n",
        "    def test(self, x, y):\n",
        "        return super().test(self.generate_features(x, self.max_power), y)\n",
        "    \n",
        "class polynomial_regression_sklearn(sklearn_LR):\n",
        "    def __init__(self, max_power, *args, **kwargs):\n",
        "        self.max_power=max_power\n",
        "        super().__init__(*args, **kwargs)\n",
        "    generate_features=staticmethod(polynomial_regression.generate_features)\n",
        "    def fit(self, x, y):\n",
        "        super().fit(self.generate_features(x, self.max_power), y)\n",
        "    def predict(self, x):\n",
        "        return super().predict(self.generate_features(x, self.max_power))\n",
        "    def test(self, x, y):\n",
        "        return np.sum((self.predict(x) - y)**2) / y.shape[0]\n",
        "\n",
        "polynomial_regression.generate_features(np.array([1, 2, 3, 4]), 4)\n",
        "\n",
        "#Построение полиномиальной регресси при помощи реализации линейной регрессии из Sklearn\n",
        "mod1=sklearn_LR()\n",
        "mod1.fit(polynomial_regression.generate_features(x_data, 3), f_data)\n",
        "mod1.predict(polynomial_regression.generate_features(x_data, 3))\n",
        "\n",
        "#Построение полиномиальной регресси при ощи созданного нами класса\n",
        "mod2=polynomial_regression_sklearn(3)\n",
        "mod2.fit(x_data, f_data)\n",
        "mod2.predict(x_data)\n",
        "\n",
        "our_mod=polynomial_regression(2)\n",
        "our_mod.fit(x_data, f_data)\n",
        "sk_mod=polynomial_regression_sklearn(2)\n",
        "sk_mod.fit(x_data, f_data)\n",
        "print('Коэффициенты при факторах(наша модель):', our_mod.w)\n",
        "print('Коэффициенты при факторах(sklearn):', sk_mod.coef_)\n",
        "print('Нулевые коэффициенты(интерцепты):',our_mod.w0, sk_mod.intercept_)\n",
        "\n",
        "our_mod.predict(x_data)\n",
        "\n",
        "sk_mod.predict(x_data)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "model1 = polynomial_regression(1)\n",
        "model1.fit(x_data, f_data)\n",
        "plt.plot(dots, model1.predict(dots), color='r')\n",
        "\n",
        "#Sklearn\n",
        "model2 = polynomial_regression_sklearn(1)\n",
        "model2.fit(x_data, f_data)\n",
        "plt.plot(dots, model2.predict(dots), 'm--', linewidth=2)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "model1 = polynomial_regression(2)\n",
        "model1.fit(x_data, f_data)\n",
        "plt.plot(dots, model1.predict(dots), color='r')\n",
        "\n",
        "#Sklearn\n",
        "model2 = polynomial_regression_sklearn(2)\n",
        "model2.fit(x_data, f_data)\n",
        "plt.plot(dots, model2.predict(dots), 'm--', linewidth=2)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "model1 = polynomial_regression(3)\n",
        "model1.fit(x_data, f_data)\n",
        "plt.plot(dots, model1.predict(dots), color='r')\n",
        "\n",
        "#Sklearn\n",
        "model2 = polynomial_regression_sklearn(3)\n",
        "model2.fit(x_data, f_data)\n",
        "plt.plot(dots, model2.predict(dots), 'm--', linewidth=2)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "model1 = polynomial_regression(4, 0.01)\n",
        "model1.fit(x_data, f_data)\n",
        "plt.plot(dots, model1.predict(dots), color='r')\n",
        "\n",
        "#Sklearn\n",
        "model2 = polynomial_regression_sklearn(4)\n",
        "model2.fit(x_data, f_data)\n",
        "plt.plot(dots, model2.predict(dots), 'm--', linewidth=2)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "model1 = polynomial_regression(6, 0.0005, max_iter = 1e5)\n",
        "model1.fit(x_data, f_data)\n",
        "plt.plot(dots, model1.predict(dots), color='r')\n",
        "\n",
        "#Sklearn\n",
        "model2 = polynomial_regression_sklearn(6)\n",
        "model2.fit(x_data, f_data)\n",
        "plt.plot(dots, model2.predict(dots), 'm--', linewidth=2)\n",
        "\n",
        "#Значение штрафной функции для нашей модели и для модели Sklearn\n",
        "model1.test(x_data, f_data), model2.test(x_data, f_data)\n",
        "\n",
        "#Истиные коэффициенты используемые для генерации данных\n",
        "w_true=np.array([[- 13.2,  - 5.3,  - 4.17, 0, 0, 0]])\n",
        "w0_true=0.6\n",
        "\n",
        "#Коэффициенты из Sklearn\n",
        "w_sklearn=model2.coef_[np.newaxis, :]\n",
        "w0_sklearn=model2.intercept_\n",
        "\n",
        "x=polynomial_regression.generate_features(x_data, 6)\n",
        "y=f_data[:, np.newaxis]\n",
        "n_samples=len(f_data)\n",
        "\n",
        "x_dots=np.arange(-0.5, 1.55, 0.05)\n",
        "def _mserror(w, w0):\n",
        "    diff=x.dot(w.T)+w0-y\n",
        "    return np.sum(diff**2)/n_samples\n",
        "\n",
        "mserror_values=np.array([_mserror(w_true+(w_sklearn-w_true)*i, w0_true+(w0_sklearn-w0_true)*i) \n",
        "                         for i in x_dots])\n",
        "#Актуальные данные\n",
        "plt.ylabel('mserror')\n",
        "plt.xticks([0, 1], ['Истиные\\nвеса', 'Веса\\nSklearn'])\n",
        "plt.plot(x_dots, mserror_values)\n",
        "\n",
        "### Ремарка: как выполнить оптимизацию при помощи scipy\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "class polynomial_regression_lbfgs(polynomial_regression):\n",
        "    def __init__(self, max_power):\n",
        "        self.max_power=max_power\n",
        "    def _optimize(self, X, Y):\n",
        "        def assign_w(w):\n",
        "            self.w0=w[0]\n",
        "            self.w=w[1:][np.newaxis, :]\n",
        "        def func(w):\n",
        "            assign_w(w)\n",
        "            #print(self.w0, self.w)\n",
        "            #print('scipy', w)\n",
        "            return super(polynomial_regression, self).test(X, Y)\n",
        "        def jac(w):\n",
        "            assign_w(w)\n",
        "            gr_w, gr_w0=self._mserror_grad(X, Y)\n",
        "            return np.concatenate([[gr_w0], gr_w.flatten()])\n",
        "        w=np.zeros((X.shape[1]+1,))\n",
        "        w=minimize(func, w, jac=jac, method='BFGS').x\n",
        "        assign_w(w)\n",
        "        \n",
        "\n",
        "model_bfgs=polynomial_regression_lbfgs(6)\n",
        "model_bfgs.fit(x_data, f_data)\n",
        "model_bfgs.test(x_data, f_data)\n",
        "\n",
        "#Актуальные данные\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.ylim(-5000, 5000)\n",
        "plt.xlim(-10,10)\n",
        "plt.plot(dots, f(dots), color='g')\n",
        "plt.scatter(x_data, f_data)\n",
        "\n",
        "#Наша модель\n",
        "plt.plot(dots, model_bfgs.predict(dots), color='r')\n",
        "\n",
        "## Кросс-валидация\n",
        "\n",
        "![SegmentLocal](cross-validation.gif \"segment\")\n",
        "\n",
        "#Посчитаем р-квадрат\n",
        "def r_square(y, y_real):\n",
        "    ss_total=np.sum((y_real-np.mean(y_real))**2)\n",
        "    ss_res=np.sum((y_real-y)**2)\n",
        "    return 1-ss_res/ss_total\n",
        "print(r_square(model2.predict(x_data), f_data))\n",
        "\n",
        "indexes=np.arange(len(f_data))%5\n",
        "y_actual=[]\n",
        "y_predicted=[]\n",
        "\n",
        "for i in range(5):\n",
        "    y_actual.append(f_data[indexes==i])\n",
        "    mod=polynomial_regression_lbfgs(6)\n",
        "    mod.fit(x_data[indexes!=i], f_data[indexes!=i])\n",
        "    y_predicted.append(mod.predict(x_data[indexes==i]))\n",
        "    \n",
        "y_actual=np.concatenate(y_actual)\n",
        "y_predicted=np.concatenate(y_predicted)\n",
        "print(r_square(y_predicted, y_actual))\n",
        "\n",
        "np.sum((y_actual-np.mean(y_actual))**2)/len(y_actual)\n",
        "\n",
        "<b>Домашнее задание</b>\n",
        "\n",
        "1.У вас, с прошлого урока, имеются реализации расчёта среднеквадратичной ошибки и её градиента для линейнй регрессии с коэффициентами при факторах(w) и свободным коэффициентам.\n",
        "\n",
        "```python\n",
        "class linear_regression:\n",
        "    def _mserror(self, X, y_real):\n",
        "        #рассчёт среднеквадратичной ошибки\n",
        "        y = X.dot(self.w.T)+self.w0\n",
        "        return np.sum((y - y_real)**2) / y_real.shape[0]\n",
        "    def _mserror_grad(self, X, y_real):\n",
        "        #рассчёт градиента ошибки.\n",
        "        #2*delta.T.dot(X)/y_real.shape[0] - градиент по коэффициентам при факторах\n",
        "        #np.sum(2*delta)/y_real.shape[0] - производная(градиент) при нулевом коэффициенте\n",
        "        delta=(X.dot(self.w.T)+self.w0-y_real)\n",
        "        return 2*delta.T.dot(X)/y_real.shape[0], np.sum(2*delta)/y_real.shape[0]\n",
        "```\n",
        "\n",
        "В этом задании вы должны модифицировать реализацию рассчёта среднеквадратичной ошибки и рассчёта её производной, так, чтобы с к среднеквадратичной ошибке добавлялась l2 регулярязационная поправка: $ +c*\\sum \\limits _{j}  w_{j}^2 $ а к градиенту- ссответствующее выражение для градиента регулярязационной поправки.\n",
        "\n",
        "2.На основе этих функций создайте свою регуляризированную полиномиальную регрессию и опробуйте на одном из примеров построения полиномиальной модели из этого урока. <br>\n",
        "<i><b>Пояснение:</b>Для этого Вам достаточно создать класс, который наследуется от класса polynomial_regression из данного урока, и переопределить в нём методы mserror, mserror_grad(под переопределением подразумевается создание на новом классе методов с таким же названием). </i>\n",
        "\n",
        "3*. (по желанию). Оцените оцените предсказательную способность реализованного метода с использование 5-ти ступенчатой кросс-валидации, показанной в примере выше. \n",
        "\n",
        "## Масштабируемость и стохастический градиент\n",
        "\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "data, target, coef = datasets.make_regression(n_samples=1000, n_features = 2, n_informative = 2, \n",
        "                                              n_targets = 1, \n",
        "                                              noise = 5, coef = True, random_state = 2)\n",
        "target=target[:, np.newaxis]\n",
        "\n",
        "# Получим средние значения и стандартное отклонение по столбцам\n",
        "\n",
        "means = np.mean(data, axis=0)\n",
        "stds = np.std(data, axis=0)\n",
        "# параметр axis указывается для вычисления значений по столбцам, а не по всему массиву\n",
        "#(см. документацию в разделе источников)\n",
        "\n",
        "# вычтем каждое значение признака из среднего и поделим на стандартное отклонение\n",
        "for i in range(data.shape[0]):\n",
        "    for j in range(data.shape[1]):\n",
        "        data[i][j] = (data[i][j] - means[j])/stds[j]\n",
        "\n",
        "# реализуем функцию, определяющую среднеквадратичную ошибку и стохастический градиент\n",
        "def mserror(X, w, y_real):\n",
        "    y = X.dot(w.T)\n",
        "    return np.sum((y - y_real)**2) / y_real.shape[0]\n",
        "\n",
        "def mserror_stohastic_grad(X, w, y_real):\n",
        "    i=np.random.randint(X.shape[0])\n",
        "    x=X[i:i+1, :]\n",
        "    #print(x.shape, w.shape)\n",
        "    return 2*(x.dot(w.T)-y_real[i:i+1, :]).T.dot(x)\n",
        "\n",
        "# инициализируем начальный вектор весов\n",
        "w = np.zeros((1, 2))\n",
        "\n",
        "# список векторов весов после каждой итерации\n",
        "w_list = [w.flatten()]\n",
        "\n",
        "# список значений ошибок после каждой итерации\n",
        "errors = []\n",
        "\n",
        "# шаг градиентного спуска\n",
        "eta = 0.0001\n",
        "\n",
        "# максимальное число итераций\n",
        "max_iter = 1e5\n",
        "\n",
        "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
        "min_weight_dist = 1e-8\n",
        "\n",
        "# зададим начальную разницу весов большим числом\n",
        "weight_dist = np.inf\n",
        "\n",
        "# счетчик итераций\n",
        "iter_num = 0\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "# ход градиентного спуска\n",
        "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
        "    \n",
        "    # генерируем случайный индекс объекта выборки\n",
        "    \n",
        "    new_w = w -  eta * mserror_stohastic_grad(data, w, target)\n",
        "\n",
        "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
        "    \n",
        "    w_list.append(new_w.flatten())\n",
        "    error=mserror(data, new_w, target)\n",
        "    errors.append(error)\n",
        "    if iter_num %1000==0:\n",
        "        print(error)\n",
        "    \n",
        "    iter_num += 1\n",
        "    w = new_w\n",
        "    \n",
        "w_list = np.array(w_list)\n",
        "\n",
        "print(f'В случае использования стохастического градиентного спуска функционал ошибки составляет {round(errors[-1], 4)}')\n",
        "\n",
        "# Визуализируем изменение весов (красной точкой обозначены истинные веса, сгенерированные вначале)\n",
        "plt.figure(figsize=(13, 6))\n",
        "plt.title('Stochastic gradient descent')\n",
        "plt.xlabel(r'$w_1$')\n",
        "plt.ylabel(r'$w_2$')\n",
        "\n",
        "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
        "plt.scatter(coef[0], coef[1], c='r')\n",
        "plt.plot(w_list[:, 0], w_list[:, 1])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Визуализируем изменение функционала ошибки\n",
        "plt.plot(range(len(errors)), errors)\n",
        "plt.title('MSE')\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "Mini-batch"
      ]
    }
  ]
}